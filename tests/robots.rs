use krik::generator::robots::generate_robots;
use krik::site::SiteConfig;
use std::fs;
use std::path::Path;
use krik::robots::generate_robots_content;

#[test]
fn robots_includes_sitemap_and_defaults() {
    let mut cfg = SiteConfig::default();
    cfg.base_url = Some("https://example.com".into());
    let out = std::env::temp_dir().join(format!("krik_test_robots_{}", std::process::id()));
    let _ = fs::remove_dir_all(&out);
    fs::create_dir_all(&out).unwrap();
    generate_robots(&cfg, Path::new(&out)).unwrap();
    let txt = fs::read_to_string(out.join("robots.txt")).unwrap();
    assert!(txt.contains("Sitemap: https://example.com/sitemap.xml"));
}

#[test]
fn test_generate_robots_content_with_base_url() {
    let mut site_config = SiteConfig::default();
    site_config.title = Some("Test Site".to_string());
    site_config.base_url = Some("https://example.com".to_string());

    let content = generate_robots_content(&site_config);

    assert!(content.contains("User-agent: *"));
    assert!(content.contains("Allow: /"));
    assert!(content.contains("Sitemap: https://example.com/sitemap.xml"));
    assert!(content.contains("Disallow: /.*"));
    assert!(content.contains("Crawl-delay: 1"));
    assert!(content.contains("Generated by Krik"));
}

#[test]
fn test_generate_robots_content_without_base_url() {
    let site_config = SiteConfig::default();

    let content = generate_robots_content(&site_config);

    assert!(content.contains("User-agent: *"));
    assert!(content.contains("Allow: /"));
    assert!(content.contains("Sitemap: /sitemap.xml"));
    assert!(!content.contains("Sitemap: https://"));
}

#[test]
fn test_robots_contains_common_disallows() {
    let site_config = SiteConfig::default();
    let content = generate_robots_content(&site_config);

    assert!(content.contains("Disallow: /.*"));
    assert!(content.contains("Disallow: /_*"));
    assert!(content.contains("Disallow: /*.tmp$"));
    assert!(content.contains("Disallow: /*.bak$"));
    assert!(content.contains("Disallow: /*.log$"));
}